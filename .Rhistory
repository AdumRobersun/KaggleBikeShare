step_time(datetime, features="hour") %>%
step_rm(datetime)
library(glmtoolbox)
library(glmnet)
bikeTrain$weather <- as.factor(bikeTrain$weather)
bikeTest$weather <- as.factor(bikeTest$weather)
bikeTrain$weather <- as.factor(bikeTrain$weather)
bikeTest$weather <- as.factor(bikeTest$weather)
bike_penalized_recipe <- recipe(count~., data=bikeTrain) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_mutate(season=factor(season, levels=1:4, labels=c("Spring", "Summer", "Fall", "Winter"))) %>%
#step_mutate(holiday=factor(holiday, levels=c(0,1), labels=c("No", "Yes"))) %>%
step_mutate(workingday=factor(workingday,levels=c(0,1), labels=c("No", "Yes"))) %>%
step_time(datetime, features="hour") %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>% #make dummy variables
step_normalize(all_numeric_predictors()) #make mean 0, sd 1
prepped_penazlied_recipe <- prep(bike_penalized_recipe)
bake(prepped_penazlied_recipe, new_data = bikeTrain)
bake(prepped_penazlied_recipe, new_data = bikeTest)
##
##
preg_model <- linear_reg(penalty = 2, mixture = 0.5) %>% #set model and tuning
set_engine("glmnet") #function to fit in R
preg_wf<- workflow() %>%
add_recipe(bike_penalized_recipe) %>%
add_model(preg_model) %>%
fit(data=bikeTrain)
#####TUNING the MODEL######
library(poissonreg)
preg_model_tune <- linear_reg(penalty=tune(),
mixture=tune()) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
#set workflow
preg_tune_wf <- workflow() %>%
add_recipe(prepped_penazlied_recipe) %>%
add_model(preg_model_tune)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 25) ## L^2 total tuning possibilities
View(bikeTest)
View(bikeTrain)
## Split data for CV
folds <- vfold_cv(bikeTrain, v = 5, repeats=1)
# Run the CV
CV_results <- preg_tune_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best("rmse")
# Finalize the Workflow & fit it
final_tuned_wf <-
preg_tune_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bikeTrain)
# Predict
final_wf %>%
predict(new_data = bikeTest)
# Predict
final_tuned_wf %>%
predict(new_data = bikeTest)
View(bikeTest)
# Predict
tuned_test_preds <- final_tuned_wf %>%
predict(new_data = bikeTest)
tuned_test_preds <- exp(tuned_test_preds)
head(tuned_test_preds)
vroom_write(x=tuned_test_preds, file="./TunedTestPreds.csv", delim=",")
# Predict
tuned_test_preds <- final_tuned_wf %>%
predict(new_data = bikeTest)%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
tuned_test_preds <- exp(tuned_test_preds)
head(tuned_test_preds)
tuned_test_preds$count <- exp(tuned_test_preds$count)
vroom_write(x=tuned_test_preds, file="./TunedTestPreds.csv", delim=",")
install.packages("raprt")
library(tidyverse)
tree_model <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n = tune()) %>%       #type of model
set_engine("rpart") %>%     #the engine is "what R function to use"
set_mode("regression")
##Create a workflow with the tree_model and recipe
reg_tree_wf <- workflow() %>%
add_recipe(prepped_penazlied_recipe) %>%
add_model(tree_model)
tree_tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 3) ## L^2 total tuning possibilities
View(bikeTrain)
##K fold CV
tree_folds <- vfold_cv(bikeTrain, v = 3, repeats=1)
# Run the CV
Tree_CV_results <- reg_tree_wf %>%
tune_grid(resamples=folds,
grid=tree_tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Find best tuning parameters
bestTreeTune <- Tree_CV_results %>%
select_best("rmse")
##K fold CV
tree_folds <- vfold_cv(bikeTrain, v = 3, repeats=1)
# Run the CV
Tree_CV_results <- reg_tree_wf %>%
tune_grid(resamples=folds,
grid=tree_tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
# Run the CV
Tree_CV_results <- reg_tree_wf %>%
tune_grid(resamples=folds,
grid=tree_tuning_grid
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
# Run the CV
Tree_CV_results <- reg_tree_wf %>%
tune_grid(resamples=folds,
grid=tree_tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
tree_tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 3) ## L^2 total tuning possibilities
##K fold CV
tree_folds <- vfold_cv(bikeTrain, v = 3, repeats=1)
# Run the CV
Tree_CV_results <- reg_tree_wf %>%
tune_grid(resamples=folds,
grid=tree_tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Find best tuning parameters
bestTreeTune <- Tree_CV_results %>%
select_best("rmse")
# Run the CV
Tree_CV_results <- reg_tree_wf %>%
tune_grid(resamples=folds,
grid=tree_tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
tree_tuning_grid <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 3) ## L^2 total tuning possibilities
##K fold CV
tree_folds <- vfold_cv(bikeTrain, v = 3, repeats=1)
# Run the CV
Tree_CV_results <- reg_tree_wf %>%
tune_grid(resamples=folds,
grid=tree_tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
## Find best tuning parameters
bestTreeTune <- Tree_CV_results %>%
select_best("rmse")
## Find best tuning parameters
bestTreeTune <- Tree_CV_results %>%
select_best("rmse")
bestTreeTune <- Tree_CV_results %>%
select_best("rmse")
install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
# Finalize the Workflow & fit it
final_tuned_tree_wf <-
reg_tree_wf %>%
finalize_workflow(bestTreeTune) %>%
fit(data=bikeTrain)
##################
#REGRESSION TREES#
##################
library(tidyverse)
library(rpart)
# Finalize the Workflow & fit it
final_tuned_tree_wf <-
reg_tree_wf %>%
finalize_workflow(bestTreeTune) %>%
fit(data=bikeTrain)
library(tidymodels)
library(vroom)
library(poissonreg)
library(glmnet)
# Finalize the Workflow & fit it
final_tuned_tree_wf <-
reg_tree_wf %>%
finalize_workflow(bestTreeTune) %>%
fit(data=bikeTrain)
# Predict
tree_test_preds <- final_tuned_tree_wf %>%
predict(new_data = bikeTest)%>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
tree_test_preds <- exp(tuned_test_preds)
tree_test_preds$count <- exp(tuned_test_preds$count)
vroom_write(x=tuned_test_preds, file="./TreeTestPreds.csv", delim=",")
vroom_write(x=tree_test_preds, file="./TreeTestPreds.csv", delim=",")
####random forests####
install.packages("ranger")
library(ranger)
library(rpart)
myrandforestmodel <- rand_forest(mtry = tune(),
min_n = tune(),
trees = 700 or 1000) %>%
myrandforestmodel <- rand_forest(mtry = tune(),
min_n = tune(),
trees =500) %>%
set_engine("ranger") %>%
set_mode("regression")
#create a workflow with model and recipe
randomforestworkflow <- workflow() %>%
add_recipe(prepped_penazlied_recipe) %>%
add_model(myrandforestmodel)
#set up grid of tuning values
random_tuning_grid <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 16) ## L^2 total tuning possibilities
#k-fold CV
random_tree_folds <- vfold_cv(bikeTrain, v = 3, repeats=1)
#and run cross validation:
# Run the CV
RandomTree_CV_results <- randomforestworkflow %>%
tune_grid(resamples=folds,
grid=random_tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
#set up grid of tuning values
random_tuning_grid <- grid_regular(mtry(),
min_n(),
levels = 16) ## L^2 total tuning possibilities
myrandforestmodel <- rand_forest(
min_n = tune(),
trees =500) %>%
set_engine("ranger") %>%
set_mode("regression")
#create a workflow with model and recipe
randomforestworkflow <- workflow() %>%
add_recipe(prepped_penazlied_recipe) %>%
add_model(myrandforestmodel)
myrandforestmodel <- rand_forest(mtry = tune()
min_n = tune(),
setwd("Desktop/STAT348/KaggleBikeShare/")
train=read.csv("train_bike.csv")
train=read.csv("train.csv")
train=read.csv("train.csv")
test=read.csv("test.csv")
test$registered=0
test$casual=0
test$count=0
data=rbind(train,test)
data$season=as.factor(data$season)
data$weather=as.factor(data$weather)
data$holiday=as.factor(data$holiday)
data$workingday=as.factor(data$workingday)
data$hour=substr(data$datetime,12,13)
data$hour=as.factor(data$hour)
train=data[as.integer(substr(data$datetime,9,10))<20,]
test=data[as.integer(substr(data$datetime,9,10))>19,]
date=substr(data$datetime,1,10)
days<-weekdays(as.Date(date))
data$day=days
data$year=substr(data$datetime,1,4)
data$year=as.factor(data$year)
train=data[as.integer(substr(data$datetime,9,10))<20,]
test=data[as.integer(substr(data$datetime,9,10))>19,]
boxplot(train$count~train$year,xlab="year", ylab="count")
library(rpart)
install.packages("rattle")
library(rattle)
library(rpart.plot)
library(RColorBrewer)
d=rpart(registered~hour,data=train)
d=rpart(registered~hour,data=train)
fancyRpartPlot(d)
data=rbind(train,test)
data$dp_reg=0
data$dp_reg[data$hour<8]=1
data$dp_reg[data$hour>=22]=2
data$dp_reg[data$hour>9 & data$hour<18]=3
data$dp_reg[data$hour==8]=4
data$dp_reg[data$hour==9]=5
data$dp_reg[data$hour==20 | data$hour==21]=6
data$dp_reg[data$hour==19 | data$hour==18]=7
data$year_part[data$year=='2011']=1
data$year_part[data$year=='2011' & data$month>3]=2
data$year_part[data$year=='2011' & data$month>6]=3
data$year_part[data$year=='2011' & data$month>9]=4
data$year_part[data$year=='2012']=5
data$year_part[data$year=='2012' & data$month>3]=6
data$year_part[data$year=='2012' & data$month>6]=7
data$year_part[data$year=='2012' & data$month>9]=8
table(data$year_part)
data$day_type=""
data$day_type[data$holiday==0 & data$workingday==0]="weekend"
data$day_type[data$holiday==1]="holiday"
data$day_type[data$holiday==0 & data$workingday==1]="working day"
data$weekend=0
data$weekend[data$day=="Sunday" | data$day=="Saturday" ]=1
train$hour=as.factor(train$hour)
test$hour=as.factor(test$hour)
y1=log(casual+1) and y2=log(registered+1)
y1=log(casual+1)
set.seed(415)
fit1 <- randomForest(logreg ~ hour +workingday+day+holiday+ day_type +temp_reg+humidity+atemp+windspeed+season+weather+dp_reg+weekend+year+year_part, data=train,importance=TRUE, ntree=250)
library(randomForest)
set.seed(415)
fit1 <- randomForest(logreg ~ hour +workingday+day+holiday+ day_type +temp_reg+humidity+atemp+windspeed+season+weather+dp_reg+weekend+year+year_part, data=train,importance=TRUE, ntree=250)
train$logreg <- log(registered)
View(train)
traincasual <- train %>%
select(-count, - registered)%>%
mutate(casual = log(casual))
library(tidyverse)
traincasual <- train %>%
select(-count, - registered)%>%
mutate(casual = log(casual))
trainregistered <- train%>%
select(-count, -casual)%>%
mutate(registered = registered)
#loading the required libraries
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
# reading the data files
train=read.csv("train_bike.csv")
#loading the required libraries
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
# reading the data files
train=read.csv("train.csv")
test=read.csv("test.csv")
str(train)
# introducing variables in test to combine train and test
# can also be done by removing the same variables from training data
test$registered=0
test$casual=0
test$count=0
data=rbind(train,test)
# factoring some variables from numeric
data$season=as.factor(data$season)
data$weather=as.factor(data$weather)
data$holiday=as.factor(data$holiday)
data$workingday=as.factor(data$workingday)
# extracting hour from the datetime variable
data$hour=substr(data$datetime,12,13)
data$hour=as.factor(data$hour)
# dividing again into train and test
train=data[as.integer(substr(data$datetime,9,10))<20,]
test=data[as.integer(substr(data$datetime,9,10))>19,]
# creating some boxplots on the count of rentals
boxplot(train$count~train$hour,xlab="hour", ylab="count of users")
boxplot(train$casual~train$hour,xlab="hour", ylab="casual users")
boxplot(train$registered~train$hour,xlab="hour", ylab="registered users")
# extracting days of week from datetime
date=substr(data$datetime,1,10)
days<-weekdays(as.Date(date))
data$day=days
train=data[as.integer(substr(data$datetime,9,10))<20,]
test=data[as.integer(substr(data$datetime,9,10))>19,]
# creating boxplots for rentals with different variables to see the variation
boxplot(train$registered~train$day,xlab="day", ylab="registered users")
boxplot(train$casual~train$day,xlab="day", ylab="casual users")
# extracting year from data
data$year=substr(data$datetime,1,4)
data$year=as.factor(data$year)
# ignore the division of data again and again, this could have been done together also
train=data[as.integer(substr(data$datetime,9,10))<20,]
test=data[as.integer(substr(data$datetime,9,10))>19,]
# again some boxplots with different variables
# these boxplots give important information about the dependent variable with respect to the independent variables
boxplot(train$registered~train$year,xlab="year", ylab="registered users")
data$hour=as.integer(data$hour)
# created this variable to divide a day into parts, but did not finally use it
data$day_part=0
train=data[as.integer(substr(data$datetime,9,10))<20,]
test=data[as.integer(substr(data$datetime,9,10))>19,]
data=rbind(train,test)
#using decision trees for binning some variables, this was a really important step in feature engineering
d=rpart(registered~hour,data=train)
fancyRpartPlot(d)
d=rpart(casual~hour,data=train)
fancyRpartPlot(d)
data=rbind(train,test)
data$dp_reg=0
data$dp_reg[data$hour<8]=1
data$dp_reg[data$hour>=22]=2
data$dp_reg[data$hour>9 & data$hour<18]=3
data$dp_reg[data$hour==8]=4
data$dp_reg[data$hour==9]=5
data$dp_reg[data$hour==20 | data$hour==21]=6
data$dp_reg[data$hour==19 | data$hour==18]=7
data$dp_cas=0
data$dp_cas[data$hour<=8]=1
data$dp_cas[data$hour==9]=2
data$dp_cas[data$hour>=10 & data$hour<=19]=3
data$dp_cas[data$hour>19]=4
f=rpart(registered~temp,data=train)
fancyRpartPlot(f)
f=rpart(casual~temp,data=train)
fancyRpartPlot(f)
data$temp_reg=0
data$temp_reg[data$temp<13]=1
data$temp_reg[data$temp>=13 & data$temp<23]=2
data$temp_reg[data$temp>=23 & data$temp<30]=3
data$temp_reg[data$temp>=30]=4
data$temp_cas=0
data$temp_cas[data$temp<15]=1
data$temp_cas[data$temp>=15 & data$temp<23]=2
data$temp_cas[data$temp>=23 & data$temp<30]=3
data$temp_cas[data$temp>=30]=4
data$year_part[data$year=='2011']=1
data$year_part[data$year=='2011' & data$month>3]=2
data$year_part[data$year=='2011' & data$month>6]=3
data$year_part[data$year=='2011' & data$month>9]=4
data$year_part[data$year=='2012']=5
data$year_part[data$year=='2012' & data$month>3]=6
data$year_part[data$year=='2012' & data$month>6]=7
data$year_part[data$year=='2012' & data$month>9]=8
table(data$year_part)
# creating another variable day_type which may affect our accuracy as weekends and weekdays are important in deciding rentals
data$day_type=0
data$day_type[data$holiday==0 & data$workingday==0]="weekend"
data$day_type[data$holiday==1]="holiday"
data$day_type[data$holiday==0 & data$workingday==1]="working day"
train=data[as.integer(substr(data$datetime,9,10))<20,]
test=data[as.integer(substr(data$datetime,9,10))>19,]
plot(train$temp,train$count)
data=rbind(train,test)
data$month=substr(data$datetime,6,7)
data$month=as.integer(data$month)
# dividing total data depending on windspeed to impute/predict the missing values
table(data$windspeed==0)
# dividing total data depending on windspeed to impute/predict the missing values
table(data$windspeed==0)
k=data$windspeed==0
wind_0=subset(data,k)
wind_1=subset(data,!k)
# predicting missing values in windspeed using a random forest model
# this is a different approach to impute missing values rather than just using the mean or median or some other statistic for imputation
set.seed(415)
fit <- randomForest(windspeed ~ season+weather +humidity +month+temp+ year+atemp, data=wind_1,importance=TRUE, ntree=250)
pred=predict(fit,wind_0)
wind_0$windspeed=pred
data=rbind(wind_0,wind_1)
data$weekend=0
data$weekend[data$day=="Sunday" | data$day=="Saturday"]=1
str(data)
# converting all relevant categorical variables into factors to feed to our random forest model
data$season=as.factor(data$season)
data$holiday=as.factor(data$holiday)
data$workingday=as.factor(data$workingday)
data$weather=as.factor(data$weather)
data$hour=as.factor(data$hour)
data$month=as.factor(data$month)
data$day_part=as.factor(data$dp_cas)
data$day_type=as.factor(data$dp_reg)
data$day=as.factor(data$day)
data$temp_cas=as.factor(data$temp_cas)
data$temp_reg=as.factor(data$temp_reg)
train=data[as.integer(substr(data$datetime,9,10))<20,]
test=data[as.integer(substr(data$datetime,9,10))>19,]
# log transformation for some skewed variables, which can be seen from their distribution
train$reg1=train$registered+1
train$cas1=train$casual+1
train$logcas=log(train$cas1)
train$logreg=log(train$reg1)
test$logreg=0
test$logcas=0
boxplot(train$logreg~train$weather,xlab="weather", ylab="registered users")
boxplot(train$logreg~train$season,xlab="season", ylab="registered users")
# final model building using random forest
# note that we build different models for predicting for registered and casual users
# this was seen as giving best result after a lot of experimentation
set.seed(415)
fit1 <- randomForest(logreg ~ hour +workingday+day+holiday+ day_type +temp_reg+humidity+atemp+windspeed+season+weather+dp_reg+weekend+year+year_part, data=train,importance=TRUE, ntree=250)
pred1=predict(fit1,test)
test$logreg=pred1
set.seed(415)
fit2 <- randomForest(logcas ~hour + day_type+day+humidity+atemp+temp_cas+windspeed+season+weather+holiday+workingday+dp_cas+weekend+year+year_part, data=train,importance=TRUE, ntree=250)
pred2=predict(fit2,test)
test$logcas=pred2
#creating the final submission file
test$registered=exp(test$logreg)-1
test$casual=exp(test$logcas)-1
test$count=test$casual+test$registered
s<-data.frame(datetime=test$datetime,count=test$count)
write.csv(s,file="submittion6.csv",row.names=FALSE)
fit1 <- randomForest(logreg ~ hour +workingday+day+holiday+ day_type +temp_reg+humidity+atemp+windspeed+season+weather+dp_reg+weekend+year+year_part, data=train,importance=TRUE, ntree=230)
pred1=predict(fit1,test)
test$logreg=pred1
set.seed(415)
fit2 <- randomForest(logcas ~hour + day_type+day+humidity+atemp+temp_cas+windspeed+season+weather+holiday+workingday+dp_cas+weekend+year+year_part, data=train,importance=TRUE, ntree=230)
pred2=predict(fit2,test)
test$logcas=pred2
#creating the final submission file
test$registered=exp(test$logreg)-1
test$casual=exp(test$logcas)-1
test$count=test$casual+test$registered
s<-data.frame(datetime=test$datetime,count=test$count)
write.csv(s,file="submittion7.csv",row.names=FALSE)
